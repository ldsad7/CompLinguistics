{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import conll2000\n",
    "import re\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Eduard\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Скачиваем conll2000\n",
    "nltk.download('conll2000')\n",
    "conll2000.ensure_loaded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Точность теггера будем считать так же, как на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_sents, postagger):\n",
    "    errors = 0\n",
    "    length = 0\n",
    "    for sent in test_sents:\n",
    "        length += len(sent)\n",
    "        sent, real_tags = zip(*sent)\n",
    "        my_tags = postagger.tag(sent)\n",
    "        for i in range(len(my_tags)):\n",
    "            if my_tags[i][1] != real_tags[i]:\n",
    "                errors += 1\n",
    "    return 1 - errors / length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Также возпользуемся нормализатором с семинара"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNormalizer:\n",
    "    def normalize(self, counter):\n",
    "        sum_ = sum(counter.values())\n",
    "        for token in counter:\n",
    "            counter[token] /= sum_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram tagger\n",
    "Для каждого слова будем выбирать наиболее вероятный тег, учитывая общую вероятность комбинации предыдущего тега и самого тега.\n",
    "$$\n",
    "     tag(w_j) = \\arg \\max_{i \\in 1 .. |Tags| } P(w_j \\mid tag_i)*P(tag_i \\mid tag(w_{j-1}))\n",
    "$$\n",
    "Для этого нам понадобятся следующие классы:\n",
    "\n",
    "**EmissionModel**, отвечающий за вероятность $P(w_j \\mid tag_i)$, он будет хранить для каждого тега вероятности быть присвоенным тому или иному слову.\n",
    "\n",
    "**TransitionModel**, отвечающий за вероятность $P(tag_i \\mid tag(w_{j-1}))$.\n",
    "\n",
    "**BigramPOSTagger**, сопоставляющий последовательности слов последовательность тегов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Класс EmissionModel можно оставить без изменений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmissionModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        for sent in tagged_sents:\n",
    "            for word, tag in sent:\n",
    "                self.model[tag][word] += 1\n",
    "        self.add_unk_token()\n",
    "        for tag in self.model:\n",
    "            self.normalizer.normalize(self.model[tag])\n",
    "        \n",
    "    def add_unk_token(self):\n",
    "        for tag in self.model:\n",
    "            self.model[tag]['UNK'] = 0.1\n",
    "        \n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "    \n",
    "    def __getitem__(self, tag):\n",
    "        return self.model[tag]\n",
    "    \n",
    "    def __call__(self, word, tag):\n",
    "        if word not in self[tag]:\n",
    "            return self[tag]['UNK']\n",
    "        return self[tag][word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Изменим класс TransitionModel, теперь он будет хранить вероятности тегов после уже известного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionModel:\n",
    "    def __init__(self, tagged_sents, normalizer=BaseNormalizer()):\n",
    "        self.normalizer = normalizer\n",
    "        self.model = defaultdict(Counter)\n",
    "        # self.model будет хранить вероятности вида P(tag_1 | tag1): defaultdict('tag1': Counter({tag_1: 0.4, tag_2: 0.6, tag_3 : 0, ...}), 'tag_2': Counter({tag_1: 0.1, tag_2: 0.3, tag_3: 0.2, ...}), ...)\n",
    "        for sent in tagged_sents:\n",
    "            # добавим дополнительный тег, чтобы определять тег самого первого слова\n",
    "            self.model['SOL'][sent[0]] += 1 # SOL = start of line\n",
    "            for word_index, tag in enumerate(sent[1:]):\n",
    "                self.model[sent[word_index - 1]][tag] += 1\n",
    "        self.add_unk_tag() # здесь также может идти неизвестный тег, т.к. в корпусе могут быть слова не всех тегов\n",
    "        for tag in self.model:\n",
    "            self.normalizer.normalize(self.model[tag])\n",
    "\n",
    "    def add_unk_tag(self):\n",
    "        for tag in self.model:\n",
    "            self.model[tag]['UNK'] = 0.1\n",
    "\n",
    "    def tags(self):\n",
    "        return self.model.keys()\n",
    "\n",
    "    def __getitem__(self, tag):\n",
    "        return self.model[tag]\n",
    "\n",
    "    def __call__(self, tag_left, tag_right=''):\n",
    "        # P(tag_left | tag_right)\n",
    "        if not tag_right: # первый тег в предложении\n",
    "            return self.model['SOL'][tag_left]\n",
    "        return self.model[tag_right][tag_left]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Немного изменим с учётом формулы UnigramPOSTagger, чтобы получить BigramPOSTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramPOSTagger:\n",
    "    def __init__(self, emission_model, transition_model):\n",
    "        self.em = emission_model\n",
    "        self.tm = transition_model\n",
    "\n",
    "    def tag(self, sent):\n",
    "        tags = []\n",
    "        prev_tag = ''\n",
    "        for word in sent:\n",
    "            max_prob = 0\n",
    "            best_tag = 'UNK'\n",
    "            for tag in self.tm.tags():\n",
    "                prob = self.em(word, tag) * self.tm(tag, prev_tag)\n",
    "                if prob > max_prob:\n",
    "                    max_prob, best_tag = prob, tag\n",
    "            tags.append(best_tag)\n",
    "            prev_tag = best_tag\n",
    "        return list(zip(sent, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = conll2000.tagged_sents()[:8000]\n",
    "em = EmissionModel(train_sents)\n",
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_sents])\n",
    "bigram_postagger = BigramPOSTagger(em, tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество при обучении на conll2000:  0.879182156133829\n"
     ]
    }
   ],
   "source": [
    "test_sents = conll2000.tagged_sents()[8000:]\n",
    "print(\"Качество при обучении на conll2000: \", accuracy(test_sents, bigram_postagger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'NNP'),\n",
       " ('had', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_postagger.tag(\"Mary had a little dog .\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверим качество на английском корпусе UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    sample = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        sent = []\n",
    "        new_sent = 0\n",
    "        for line in f.readlines():\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                sent.append((line.split()[1], line.split()[4]))\n",
    "                new_sent = 1\n",
    "            elif new_sent == 1:\n",
    "                sample.append(sent)\n",
    "                sent = []\n",
    "                new_sent = 0\n",
    "    return (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = read_corpus('en_ewt-ud-train.conllu.txt')\n",
    "test_sample = read_corpus('en_ewt-ud-test.conllu.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('never', 'RB'),\n",
       " ('return', 'VB'),\n",
       " ('there', 'RB'),\n",
       " ('again', 'RB'),\n",
       " ('(', '-LRB-'),\n",
       " ('and', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('have', 'VBP'),\n",
       " ('some', 'DT'),\n",
       " ('serious', 'JJ'),\n",
       " ('doubts', 'NNS'),\n",
       " ('about', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('quality', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('work', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('actually', 'RB'),\n",
       " ('performed', 'VBD'),\n",
       " ('on', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('car', 'NN'),\n",
       " (')', '-RRB-'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sample[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество при обучении на UP:  0.7988604215643305\n"
     ]
    }
   ],
   "source": [
    "em = EmissionModel(train_sample)\n",
    "tm = TransitionModel([[tag for word, tag in sent] for sent in train_sample])\n",
    "bigram_postagger = BigramPOSTagger(em, tm)\n",
    "print(\"Качество при обучении на UP: \", accuracy(test_sample, bigram_postagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Качество похуже, чем на conll2000, но вместе с тем неплохое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mary', 'NNP'),\n",
       " ('had', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('dog', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_postagger.tag(\"Mary had a little dog .\".split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
