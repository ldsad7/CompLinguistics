{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('woolf.txt', 'r', encoding='utf-8') as f:\n",
    "    full_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sents = [re.sub('\\n', ' ', elem) for elem in sent_tokenize(full_text)][750:800]\n",
    "text = ' '.join(correct_sents)\n",
    "correct_sents[39:40] = re.split('(?<=\\.{3})\\s', correct_sents[39])\n",
    "correct_sents[43:44] = re.split('(?<=\\.{3})\\s', correct_sents[43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They had quarrelled.',\n",
       " 'Why the right way to open a tin of beef, with Shakespeare on board, under conditions of such splendour, should have turned them to sulky schoolboys, none can tell.',\n",
       " 'Tinned beef is cold eating, though; and salt water spoils biscuits; and the waves tumble and lollop much the same hour after hour--tumble and lollop all across the horizon.',\n",
       " 'Now a spray of seaweed floats past-now a log of wood.',\n",
       " 'Ships have been wrecked here.',\n",
       " 'One or two go past, keeping their own side of the road.',\n",
       " 'Timmy knew where they were bound, what their cargoes were, and, by looking through his glass, could tell the name of the line, and even guess what dividends it paid its shareholders.',\n",
       " 'Yet that was no reason for Jacob to turn sulky.',\n",
       " 'The Scilly Isles had the look of mountain-tops almost a-wash...',\n",
       " 'Unfortunately, Jacob broke the pin of the Primus stove.',\n",
       " 'The Scilly Isles might well be obliterated by a roller sweeping straight across.',\n",
       " 'But one must give young men the credit of admitting that, though breakfast eaten under these circumstances is grim, it is sincere enough.',\n",
       " 'No need to make conversation.',\n",
       " 'They got out their pipes.',\n",
       " 'Timmy wrote up some scientific observations; and--what was the question that broke the silence--the exact time or the day of the month?',\n",
       " 'Anyhow, it was spoken without the least awkwardness; in the most matter-of-fact way in the world; and then Jacob began to unbutton his clothes and sat naked, save for his shirt, intending, apparently, to bathe.',\n",
       " 'The Scilly Isles were turning bluish; and suddenly blue, purple, and green flushed the sea; left it grey; struck a stripe which vanished; but when Jacob had got his shirt over his head the whole floor of the waves was blue and white, rippling and crisp, though now and again a broad purple mark appeared, like a bruise; or there floated an entire emerald tinged with yellow.',\n",
       " 'He plunged.',\n",
       " 'He gulped in water, spat it out, struck with his right arm, struck with his left, was towed by a rope, gasped, splashed, and was hauled on board.',\n",
       " 'The seat in the boat was positively hot, and the sun warmed his back as he sat naked with a towel in his hand, looking at the Scilly Isles which--confound it!',\n",
       " 'The sail flapped.',\n",
       " 'Shakespeare was knocked overboard.',\n",
       " 'There you could see him floating merrily away, with all his pages ruffling innumerably; and then he went under.',\n",
       " 'Strangely enough, you could smell violets, or if violets were impossible in July, they must grow something very pungent on the mainland then.',\n",
       " 'The mainland, not so very far off--you could see clefts in the cliffs, white cottages, smoke going up--wore an extraordinary look of calm, of sunny peace, as if wisdom and piety had descended upon the dwellers there.',\n",
       " 'Now a cry sounded, as of a man calling pilchards in a main street.',\n",
       " 'It wore an extraordinary look of piety and peace, as if old men smoked by the door, and girls stood, hands on hips, at the well, and horses stood; as if the end of the world had come, and cabbage fields and stone walls, and coast-guard stations, and, above all, the white sand bays with the waves breaking unseen by any one, rose to heaven in a kind of ecstasy.',\n",
       " 'But imperceptibly the cottage smoke droops, has the look of a mourning emblem, a flag floating its caress over a grave.',\n",
       " 'The gulls, making their broad flight and then riding at peace, seem to mark the grave.',\n",
       " 'No doubt if this were Italy, Greece, or even the shores of Spain, sadness would be routed by strangeness and excitement and the nudge of a classical education.',\n",
       " 'But the Cornish hills have stark chimneys standing on them; and, somehow or other, loveliness is infernally sad.',\n",
       " 'Yes, the chimneys and the coast-guard stations and the little bays with the waves breaking unseen by any one make one remember the overpowering sorrow.',\n",
       " 'And what can this sorrow be?',\n",
       " 'It is brewed by the earth itself.',\n",
       " 'It comes from the houses on the coast.',\n",
       " 'We start transparent, and then the cloud thickens.',\n",
       " 'All history backs our pane of glass.',\n",
       " 'To escape is vain.',\n",
       " \"But whether this is the right interpretation of Jacob's gloom as he sat naked, in the sun, looking at the Land's End, it is impossible to say; for he never spoke a word.\",\n",
       " 'Timmy sometimes wondered (only for a second) whether his people bothered him...',\n",
       " 'No matter.',\n",
       " \"There are things that can't be said.\",\n",
       " \"Let's shake it off.\",\n",
       " \"Let's dry ourselves, and take up the first thing that comes handy...\",\n",
       " \"Timmy Durrant's notebook of scientific observations.\",\n",
       " '\"Now...\" said Jacob.',\n",
       " 'It is a tremendous argument.',\n",
       " 'Some people can follow every step of the way, and even take a little one, six inches long, by themselves at the end; others remain observant of the external signs.',\n",
       " 'The eyes fix themselves upon the poker; the right hand takes the poker and lifts it; turns it slowly round, and then, very accurately, replaces it.',\n",
       " 'The left hand, which lies on the knee, plays some stately but intermittent piece of march music.',\n",
       " 'A deep breath is taken; but allowed to evaporate unused.',\n",
       " 'The cat marches across the hearth-rug.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_regex = '(?<=[.!?]) (?=[A-Z])'\n",
    "first_split = [elem for elem in re.split(first_regex, text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_regex = '(?<=[.!?])\\s(?=[^a-z])'\n",
    "second_split = [elem for elem in re.split(second_regex, text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент правильных ответов по метрике с множествами (first_regex): 0.9433962264150944\n",
      "Процент правильных ответов по метрике с множествами (second_regex): 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Процент правильных ответов по метрике с множествами (first_regex): {}\".format(len(set(correct_sents) & set(first_split)) / len(set(correct_sents) | set(first_split))))\n",
    "print(\"Процент правильных ответов по метрике с множествами (second_regex): {}\".format(len(set(correct_sents) & set(second_split)) / len(set(correct_sents) | set(second_split))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"Now...\" said Jacob.',\n",
       " \"Timmy Durrant's notebook of scientific observations.\",\n",
       " 'Timmy Durrant\\'s notebook of scientific observations. \"Now...\" said Jacob.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Посмотрим с помощью операции симметрической разности, на каких предложениях ошибается первая регулярка\n",
    "set(correct_sents) ^ set(first_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как видно, первая регулярка ошибается на {. \"}, поскольку она воспринимает как начало следующего предложения только те случаи, когда предложение начинается с заглавной буквы. Вторая регулярка не страдает этим, поскольку для неё важно, чтобы следующее предложение начиналось не со строчной буквы, что верно и в этом случае."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Можно также посчитать accuracy, precision, recall и f1-меру на униграммах и биграммах (униграммы не должны разбиваться, в отличие от биграмм). Сделаем это как вручную, так и с помощью встроенных функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_table(regex):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0 # для accuracy необходимо значение и true negative\n",
    "\n",
    "    for sent in correct_sents:\n",
    "        if len(re.split(regex, sent)) == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    for i in range(len(correct_sents) - 1):\n",
    "        sent = ' '.join([correct_sents[i], correct_sents[i + 1]])\n",
    "        if len(re.split(regex, sent)) == 2:\n",
    "            tn += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print('tp: {}, fp: {}, fn: {}, tn: {}'.format(tp, fp, fn, tn))\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print('F1: ', f1)\n",
    "    print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 52, fp: 0, fn: 1, tn: 50\n",
      "Precision:  1.0\n",
      "Recall:  0.9811320754716981\n",
      "F1:  0.9904761904761905\n",
      "Accuracy:  0.9902912621359223\n"
     ]
    }
   ],
   "source": [
    "find_table(first_regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 52, fp: 0, fn: 0, tn: 51\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "F1:  1.0\n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "find_table(second_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Здесь видно, что первая регулярка не может разбить предложение 'Timmy Durrant's notebook of scientific observations. \"Now...\" said Jacob.' (fn = 1), однако fn не входит в формулу precision, поэтому precision = 1. Со второй регуляркой все метрики равны 1, поскольку ошибок нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_borders = [0 for symbol in text]\n",
    "cur_length = 0\n",
    "for sent in first_split:\n",
    "    cur_length += len(sent)\n",
    "    first_borders[cur_length - 1] = 1\n",
    "    cur_length += 1\n",
    "second_borders = [0 for symbol in text]\n",
    "cur_length = 0\n",
    "for sent in second_split:\n",
    "    cur_length += len(sent)\n",
    "    second_borders[cur_length - 1] = 1\n",
    "    cur_length += 1\n",
    "correct_borders = [0 for symbol in text]\n",
    "cur_length = 0\n",
    "for sent in correct_sents:\n",
    "    cur_length += len(sent)\n",
    "    correct_borders[cur_length - 1] = 1\n",
    "    cur_length += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(first_borders == correct_borders), np.all(second_borders == correct_borders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = np.where(np.equal(first_borders, correct_borders) == False)[0][0]\n",
    "first_borders[index], correct_borders[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4899\n",
      "          1       1.00      0.98      0.99        52\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(correct_borders, first_borders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4899\n",
      "          1       1.00      1.00      1.00        52\n",
      "\n",
      "avg / total       1.00      1.00      1.00      4951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(correct_borders, second_borders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Здесь мы рассмотрели задачу определения границ предложения как задачу классификации: в последовательности символов единицей закодированы те, что на границе предложений, а нулями -- все остальные. Precision в случае 0 first_borders не равна ровно 1, а скорее равна 4898/4899, но, видимо, здесь это значение округляется, поскольку оно очень близко к 1. Соответственно, это верно и для f1-score в случае 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "\"\"\"\n",
    "this includes as potential collocations all word pairs where the first\n",
    "word ends in a period. It may be useful in corpora where there is a lot\n",
    "of variation that makes abbreviations like Mr difficult to identify.\n",
    "\"\"\"\n",
    "trainer.train((' '.join(full_text.split())).replace(text, '')) # удалили участок, на котором будем тестировать\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((';', ':', ',', '.', '!', '?'),\n",
       " {'1.a',\n",
       "  '1.b',\n",
       "  '1.d',\n",
       "  '1.e.2',\n",
       "  '1.e.3',\n",
       "  '1.e.4',\n",
       "  '1.e.5',\n",
       "  '1.e.6',\n",
       "  '1.e.9',\n",
       "  '1.f',\n",
       "  '1.f.1',\n",
       "  '1.f.2',\n",
       "  '1.f.4',\n",
       "  '1.f.5',\n",
       "  '1.f.6',\n",
       "  'a.b.c',\n",
       "  'dr',\n",
       "  'e.m',\n",
       "  'esq',\n",
       "  'etc',\n",
       "  'f3',\n",
       "  'mr',\n",
       "  'mrs',\n",
       "  'r.b',\n",
       "  'rev',\n",
       "  's',\n",
       "  'st',\n",
       "  'u.s'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.PUNCTUATION, tokenizer._params.abbrev_types # аббревиатуры и знаки препинания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_split = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp: 52, fp: 0, fn: 2, tn: 49\n",
      "Precision:  1.0\n",
      "Recall:  0.9629629629629629\n",
      "F1:  0.9811320754716981\n",
      "Accuracy:  0.9805825242718447\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0 # для accuracy необходимо значение и true negative\n",
    "\n",
    "for sent in correct_sents:\n",
    "    if len(tokenizer.tokenize(sent)) == 1:\n",
    "        tp += 1\n",
    "    else:\n",
    "        fp += 1\n",
    "\n",
    "for i in range(len(correct_sents) - 1):\n",
    "    sent = ' '.join([correct_sents[i], correct_sents[i + 1]])\n",
    "    if len(tokenizer.tokenize(sent)) == 2:\n",
    "        tn += 1\n",
    "    else:\n",
    "        fn += 1\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "print('tp: {}, fp: {}, fn: {}, tn: {}'.format(tp, fp, fn, tn))\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1: ', f1)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Let's dry ourselves, and take up the first thing that comes handy...\",\n",
       " \"Let's dry ourselves, and take up the first thing that comes handy... Timmy Durrant's notebook of scientific observations.\",\n",
       " 'No matter.',\n",
       " \"Timmy Durrant's notebook of scientific observations.\",\n",
       " 'Timmy sometimes wondered (only for a second) whether his people bothered him...',\n",
       " 'Timmy sometimes wondered (only for a second) whether his people bothered him... No matter.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tokenizer_split) ^ set(correct_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Встроенный tokenizer ведёт себя хуже, чем first_regex и тем более чем second_regex. Видимо, проблема в том, что tokenizer не воспринимает ... (а не …) как целый разделитель. Поэтому попробуем добавить этот разделитель в tokenizer.PUNCTUATION и проверим качество:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.PUNCTUATION = tuple(list(tokenizer.PUNCTUATION) + ['...'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(';', ':', ',', '.', '!', '?', '...')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.PUNCTUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_split = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Let's dry ourselves, and take up the first thing that comes handy...\",\n",
       " \"Let's dry ourselves, and take up the first thing that comes handy... Timmy Durrant's notebook of scientific observations.\",\n",
       " 'No matter.',\n",
       " \"Timmy Durrant's notebook of scientific observations.\",\n",
       " 'Timmy sometimes wondered (only for a second) whether his people bothered him...',\n",
       " 'Timmy sometimes wondered (only for a second) whether his people bothered him... No matter.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(tokenizer_split) ^ set(correct_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Не сработало. Попробуем изменить в тексте все троеточия на один символ троеточия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = full_text.replace('...', '…')\n",
    "trainer = PunktTrainer()\n",
    "trainer.INCLUDE_ALL_COLLOCS = True\n",
    "trainer.train((' '.join(full_text.split())).replace(text, ''))\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(';', ':', ',', '.', '!', '?')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.PUNCTUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.PUNCTUATION = tuple(list(tokenizer.PUNCTUATION) + ['…'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_split = tokenizer.tokenize(text.replace('...', '…'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_sents = [elem.replace('...', '…') for elem in correct_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"Let's dry ourselves, and take up the first thing that comes handy…\",\n",
       " \"Let's dry ourselves, and take up the first thing that comes handy… Timmy Durrant's notebook of scientific observations.\",\n",
       " 'No matter.',\n",
       " 'The Scilly Isles had the look of mountain-tops almost a-wash…',\n",
       " 'The Scilly Isles had the look of mountain-tops almost a-wash… Unfortunately, Jacob broke the pin of the Primus stove.',\n",
       " \"Timmy Durrant's notebook of scientific observations.\",\n",
       " 'Timmy sometimes wondered (only for a second) whether his people bothered him…',\n",
       " 'Timmy sometimes wondered (only for a second) whether his people bothered him… No matter.',\n",
       " 'Unfortunately, Jacob broke the pin of the Primus stove.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(correct_sents) ^ set(tokenizer_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Но и это не помогает (стало ещё хуже), поскольку, по-видимому, проблема в не типографском знаке (его отсутствии), а в том, что PunktSentenceTokenizer рассматривает случаи выше как продолжение предыдущего, хотя, на мой взгляд, если бы это было действительно так, то продолжение писалось бы со строчной буквы (см. пример из BrE \"It is not cold… it is freezing cold.\" из https://en.wikipedia.org/wiki/Ellipsis). Возможно, в случае \"Timmy Durrant's notebook of scientific observations\" tokenizer размечает и правильно (трудно понять, т.к. имя всегда пишется с заглавной буквы), но в двух других случаях скорее неправильно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Покажем, что некоторые ошибки из других отрывков мы можем исправить добавлением сокращений в tokenizer._params.abbrev_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856\n"
     ]
    }
   ],
   "source": [
    "for i, sent in enumerate([re.sub('\\n', ' ', elem) for elem in sent_tokenize(full_text)]):\n",
    "    if sent.startswith('Having drawn her water'):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having drawn her water, Mrs. Pascoe went in.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_sent = [re.sub('\\n', ' ', elem) for elem in sent_tokenize(full_text)][856]\n",
    "tokenizer.tokenize(hard_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сокращение Mrs уже добавлено в tokenizer._params.abbrev_types, поэтому не сработало. Удалим из этого множества этот элемент и убедимся в том, что теоретически в каком-нибудь другом случае добавление аббревиатуры может помочь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._params.abbrev_types -= {'mrs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1.a',\n",
       " '1.b',\n",
       " '1.d',\n",
       " '1.e.2',\n",
       " '1.e.3',\n",
       " '1.e.4',\n",
       " '1.e.5',\n",
       " '1.e.6',\n",
       " '1.e.9',\n",
       " '1.f',\n",
       " '1.f.1',\n",
       " '1.f.2',\n",
       " '1.f.4',\n",
       " '1.f.5',\n",
       " '1.f.6',\n",
       " 'a.b.c',\n",
       " 'dr',\n",
       " 'e.m',\n",
       " 'esq',\n",
       " 'etc',\n",
       " 'f3',\n",
       " 'mr',\n",
       " 'r.b',\n",
       " 'rev',\n",
       " 's',\n",
       " 'st',\n",
       " 'u.s'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._params.abbrev_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having drawn her water, Mrs. Pascoe went in.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(hard_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Думаю, проблема в том, что токенизатор уже выучил данное предложение. Попробуем обучить его без этого предложения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = full_text.replace('...', '…')\n",
    "trainer = PunktTrainer()\n",
    "trainer.train((' '.join(full_text.split())).replace(text, '').replace(\"Having drawn her water, Mrs. Pascoe went in.\", \"\"))\n",
    "\n",
    "tokenizer = PunktSentenceTokenizer(trainer.get_params())\n",
    "tokenizer._params.abbrev_types -= {'mrs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having drawn her water, Mrs.', 'Pascoe went in.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(hard_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Having drawn her water, Mrs. Pascoe went in.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._params.abbrev_types |= {'mrs'}\n",
    "tokenizer.tokenize(hard_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
